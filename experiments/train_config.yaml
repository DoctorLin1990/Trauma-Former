# Trauma-Former Training Configuration
# This file contains all hyperparameters for model training

# Experiment metadata
experiment:
  name: "Trauma-Former_TIC_Prediction"
  version: "1.0.0"
  description: "Training configuration for Trauma-Former model"
  timestamp: "auto"  # Will be replaced with actual timestamp
  
# Data configuration
data:
  # Paths
  train_data_path: "./data/synthetic/X_synthetic.npy"
  train_labels_path: "./data/synthetic/y_synthetic.npy"
  test_split: 0.2
  validation_split: 0.1
  
  # Data parameters
  sequence_length: 30  # 30 seconds at 1 Hz
  num_features: 4      # HR, SBP, DBP, SpO2
  features: ["heart_rate", "systolic_bp", "diastolic_bp", "spo2"]
  
  # Preprocessing
  preprocessing:
    normalization: "z_score"  # Options: z_score, min_max, none
    handle_missing: "linear_interpolation"
    filter_noise: true
    filter_type: "butterworth"
    cutoff_frequency: 0.5
    augmentation: true
    augmentation_methods: ["noise", "scaling", "time_warp"]
  
  # Batch generation
  batch_size: 32
  shuffle: true
  num_workers: 4
  pin_memory: true

# Model configuration
model:
  name: "Trauma-Former"
  type: "transformer"
  
  # Transformer parameters (as in Section 3.3)
  transformer:
    d_model: 128            # Embedding dimension
    nhead: 4                # Number of attention heads
    num_encoder_layers: 2   # Number of transformer encoder layers
    dim_feedforward: 512    # Feed-forward network dimension
    dropout: 0.1            # Dropout rate
    activation: "relu"      # Activation function
    layer_norm_eps: 1e-5    # Layer normalization epsilon
  
  # LSTM baseline (for comparison)
  lstm_baseline:
    hidden_size: 64
    num_layers: 2
    dropout: 0.2
    bidirectional: false
  
  # Positional encoding
  positional_encoding:
    type: "sinusoidal"      # Options: sinusoidal, learned
    max_len: 5000
  
  # Classification head
  classifier:
    hidden_layers: [64, 32]  # Hidden layer sizes
    dropout: 0.1
    activation: "relu"
    output_activation: "sigmoid"

# Training configuration
training:
  # Basic parameters
  epochs: 100
  early_stopping_patience: 15
  early_stopping_min_delta: 0.001
  
  # Optimization
  optimizer:
    type: "Adam"            # Options: Adam, SGD, RMSprop
    learning_rate: 0.0001
    weight_decay: 0.0001
    betas: [0.9, 0.999]     # For Adam
    momentum: 0.9           # For SGD
    eps: 1e-8
  
  # Learning rate scheduler
  scheduler:
    type: "ReduceLROnPlateau"  # Options: StepLR, CosineAnnealing, ReduceLROnPlateau
    step_size: 10            # For StepLR
    gamma: 0.5               # For StepLR
    patience: 10             # For ReduceLROnPlateau
    factor: 0.5              # For ReduceLROnPlateau
    min_lr: 1e-6             # Minimum learning rate
    verbose: true
  
  # Loss function
  loss:
    type: "BCE"              # Binary Cross Entropy
    pos_weight: 1.0          # Weight for positive class
    reduction: "mean"
  
  # Gradient handling
  gradient:
    clip_grad_norm: true
    max_norm: 1.0
    clip_grad_value: false
    clip_value: 5.0

# Evaluation configuration
evaluation:
  # Metrics to compute
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "auroc"
    - "auprc"
    - "specificity"
    - "npv"
    - "average_precision"
  
  # Threshold for binary classification
  threshold: 0.5
  
  # Early warning evaluation
  early_warning:
    enabled: true
    warning_threshold: 0.8
    min_lead_time_seconds: 60  # Minimum lead time to count as early warning
  
  # Confidence intervals
  confidence_intervals:
    enabled: true
    method: "bootstrap"        # Options: bootstrap, binomial
    n_bootstraps: 1000
    confidence_level: 0.95
  
  # ROC curve
  roc_curve:
    num_thresholds: 100
    save_plot: true
  
  # Calibration
  calibration:
    enabled: true
    method: "isotonic"         # Options: isotonic, platt
    n_bins: 10

# Logging and checkpoints
logging:
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "./logs/tensorboard"
    update_freq: 10
  
  # Console logging
  console:
    enabled: true
    log_level: "INFO"          # DEBUG, INFO, WARNING, ERROR
    progress_bar: true
  
  # File logging
  file:
    enabled: true
    log_dir: "./logs"
    log_file: "training.log"
  
  # Checkpoints
  checkpoints:
    save_best: true
    save_frequency: 10         # Save every N epochs
    checkpoint_dir: "./checkpoints"
    max_checkpoints: 5         # Keep only N best checkpoints
  
  # Metrics tracking
  metrics_tracking:
    train_metrics_frequency: 1    # Log train metrics every N epochs
    val_metrics_frequency: 1      # Log validation metrics every N epochs
    save_metrics_csv: true

# System configuration
system:
  # Hardware
  device: "cuda"              # Options: cuda, cpu, auto
  cuda_device: 0              # GPU device index
  num_gpus: 1                 # Number of GPUs to use
  
  # Memory
  memory:
    max_memory_allocated: 0.9  # Maximum GPU memory usage (0.9 = 90%)
    pin_memory: true
  
  # Performance
  performance:
    benchmark: false           # cuDNN benchmark mode
    deterministic: true        # Deterministic algorithms
    num_threads: 8            # CPU threads for data loading
  
  # Distributed training (optional)
  distributed:
    enabled: false
    backend: "nccl"           # Communication backend
    world_size: 1
    rank: 0

# Reproducibility
reproducibility:
  seed: 42
  deterministic_operations: true
  cudnn_deterministic: true
  cudnn_benchmark: false

# Paths
paths:
  # Input paths
  data_dir: "./data"
  synthetic_data_dir: "./data/synthetic"
  
  # Output paths
  output_dir: "./results"
  model_dir: "./models/saved"
  logs_dir: "./logs"
  figures_dir: "./figures"
  
  # Checkpoint paths
  checkpoint_format: "checkpoint_epoch_{epoch:04d}.pth"
  best_model_name: "best_model.pth"
  final_model_name: "final_model.pth"

# Monitoring and alerts
monitoring:
  # System monitoring
  system:
    monitor_gpu: true
    monitor_memory: true
    monitor_cpu: true
    
  # Training monitoring
  training:
    monitor_loss: true
    monitor_metrics: true
    monitor_gradients: false
    monitor_weights: false
    
  # Alerting
  alerts:
    enabled: false
    loss_increase_threshold: 2.0  # Alert if loss increases by factor of X
    metric_drop_threshold: 0.1    # Alert if metric drops by X

# Advanced options
advanced:
  # Mixed precision training
  mixed_precision:
    enabled: false
    dtype: "float16"          # Options: float16, bfloat16
  
  # Gradient accumulation
  gradient_accumulation:
    enabled: false
    steps: 4                  # Accumulate gradients over N steps
  
  # Model EMA (Exponential Moving Average)
  model_ema:
    enabled: false
    decay: 0.999
    update_every: 10
  
  # Knowledge distillation (for model compression)
  knowledge_distillation:
    enabled: false
    teacher_model_path: null
    temperature: 3.0
    alpha: 0.5                # Weight for distillation loss
  
  # Pruning (for model compression)
  pruning:
    enabled: false
    method: "l1_unstructured"
    amount: 0.2               # Percentage of weights to prune

# Validation and testing
validation:
  # Cross-validation
  cross_validation:
    enabled: false
    n_folds: 5
    stratified: true
  
  # Hyperparameter search
  hyperparameter_search:
    enabled: false
    method: "random"          # Options: grid, random, bayesian
    n_trials: 50
    param_distributions: {}   # Define parameter distributions here
  
  # Ablation studies
  ablation:
    enabled: false
    studies:
      - name: "no_attention"
        disable_attention: true
      - name: "no_positional_encoding"
        disable_positional_encoding: true
      - name: "shorter_window"
        window_size: 15

# Export and deployment
export:
  # Model export formats
  formats:
    pytorch: true            # Save as .pth
    onnx: false              # Export to ONNX format
    torchscript: false       # Export to TorchScript
    
  # ONNX export settings
  onnx:
    opset_version: 11
    dynamic_axes:
      input: {0: "batch_size", 1: "sequence_length"}
      output: {0: "batch_size"}
    
  # Deployment settings
  deployment:
    target_device: "cpu"     # Target device for deployment
    quantize: false          # Apply quantization
    optimize: true           # Apply optimization passes

# Notes and comments
notes: |
  This configuration file is used to train the Trauma-Former model
  as described in the paper "Real-Time Dynamic Prediction of Trauma-Induced
  Coagulopathy via a 5G-Enabled Digital Twin Framework".
  
  Key hyperparameters are based on the optimal values found during
  experimentation and validation.
  
  To run training with this configuration:
      python train.py --config experiments/train_config.yaml
  
  To override specific parameters from command line:
      python train.py --config experiments/train_config.yaml \
        --training.epochs 200 \
        --training.optimizer.learning_rate 0.0005